{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devparikh0506/DATA_690_Deep_Learning/blob/main/project_1/cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [source](https://github.com/bufuchangfeng/NeuralNetwork/blob/nn/nn_and_cnn.ipynb)"
      ],
      "metadata": {
        "id": "juX2QHAKBgaT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f2Np2dRGuFsb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from abc import ABC, abstractmethod"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kvbqoIZ_z9Kz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ef8570-ea6e-442d-cff1-d50a08146872"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "ecQ-oKj-DtbI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    \"\"\"Loads the MNIST dataset and splits it into training and testing sets.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the MNIST dataset file.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (x_train, y_train, x_test, y_test)\n",
        "    \"\"\"\n",
        "    with np.load(path) as f:\n",
        "        # Load training data and labels\n",
        "        x_train, y_train = f['x_train'], f['y_train']\n",
        "        # Load testing data and labels\n",
        "        x_test, y_test = f['x_test'], f['y_test']\n",
        "        return x_train, y_train, x_test, y_test\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_data, train_label, test_data, test_label = load_data('/content/drive/MyDrive/DATA-690-deep-learning/data/mnist.npz')\n"
      ],
      "metadata": {
        "id": "6GGxK7ka0mwu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = (train_data/255 - 0.5)*2\n",
        "X_test = (test_data/255 - 0.5)*2"
      ],
      "metadata": {
        "id": "Q6nd-S_46_UI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUWNTaQMA0M_",
        "outputId": "a08b5666-85f0-4b6f-adb7-21735be9593a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = X_train.reshape(-1, 1, 28, 28)"
      ],
      "metadata": {
        "id": "ar6C1Np5O92r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_label.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkgkwUaUA437",
        "outputId": "34e6c300-736e-4964-f219-a71843e35d20"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZlgPEh5A7UD",
        "outputId": "164e33da-39ef-496f-a6e3-97e016d07650"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = X_test.reshape(-1, 1, 28, 28)"
      ],
      "metadata": {
        "id": "jJUwaL8QPHaJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, valid_data, train_label, valid_label = train_test_split(train_data, train_label, test_size=0.2)"
      ],
      "metadata": {
        "id": "syoW_JzqBLJI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAteLkirBW5D",
        "outputId": "21eb5831-ea40-4252-c0be-1e54aeabf8e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(48000, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JDt5c2ZjuFsd",
        "outputId": "d5ffb0a2-5acb-4929-ece7-5a701acfb6fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.imshow(valid_data[2].reshape(28, 28))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_label[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Bkeka7RBuxv",
        "outputId": "934c8f47-90de-4c72-d414-e77aabf422d6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BgxGVU90uFsd"
      },
      "outputs": [],
      "source": [
        "def label_to_one_hot(y, n_class):\n",
        "    \"\"\"Converts labels to one-hot encoding.\n",
        "\n",
        "    Args:\n",
        "        y (numpy.ndarray): Array of labels.\n",
        "        n_class (int): Number of classes.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: One-hot encoded labels.\n",
        "    \"\"\"\n",
        "    one_hot = np.zeros((y.shape[0], n_class))\n",
        "    for i in range(len(y)):\n",
        "        one_hot[i][int(y[i])] = 1\n",
        "\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(data, label, batch_size):\n",
        "  \"\"\"\n",
        "  Generates batches of data and labels.\n",
        "\n",
        "  Args:\n",
        "    data: The input data.\n",
        "    label: The corresponding labels.\n",
        "    batch_size: The size of each batch.\n",
        "\n",
        "  Yields:\n",
        "    A tuple of (data_batch, label_batch).\n",
        "  \"\"\"\n",
        "  num_samples = len(data)\n",
        "  num_batches = num_samples // batch_size\n",
        "\n",
        "  for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = (i + 1) * batch_size\n",
        "    yield data[start:end], label[start:end]\n",
        "\n",
        "  # If there are remaining samples, yield a smaller batch.\n",
        "  if num_samples % batch_size != 0:\n",
        "    yield data[num_batches * batch_size:], label[num_batches * batch_size:]"
      ],
      "metadata": {
        "id": "kQFiantFENfg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(ABC):\n",
        "  \"\"\"The base class for NN model layer.\"\"\"\n",
        "  def __init__(self):\n",
        "      super.__init__()\n",
        "\n",
        "  @abstractmethod\n",
        "  def forward(self, x):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  @abstractmethod\n",
        "  def backward(self, dout):\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "IVZ_5IDKE1YI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iT4rJH-kuFsd"
      },
      "outputs": [],
      "source": [
        "class Linear(Layer):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.params = {}\n",
        "        self.params['W'] = np.random.randn(input_dim, output_dim) / np.sqrt(input_dim)\n",
        "        self.params['b'] = np.random.randn(output_dim)\n",
        "\n",
        "        self.grads = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(x, self.params['W']) + self.params['b']\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "\n",
        "        self.grads['W'] = np.dot(self.x.T, dout)\n",
        "        self.grads['b'] = np.sum(dout, axis=0) # further clarify\n",
        "\n",
        "        return np.dot(dout, self.params['W'].T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wB5jjKTWuFsd"
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        self.params = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "\n",
        "        return dout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "NgHRckwnFBwQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxWithCrossEntropyLoss(Layer):\n",
        "    def __init__(self):\n",
        "        self.params = None\n",
        "\n",
        "    def forward(self, out, y):\n",
        "        '''\n",
        "            out: output of last fully connected layer\n",
        "            y: true label\n",
        "        '''\n",
        "\n",
        "        batch_size = out.shape[0]\n",
        "\n",
        "        out = softmax(out)\n",
        "\n",
        "        self.out = out\n",
        "        self.y = y\n",
        "\n",
        "        log_out = np.log(out + 1e-7)\n",
        "\n",
        "        loss = np.sum(-log_out * y)\n",
        "\n",
        "        return loss / batch_size, out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return (self.out - self.y) / batch_size"
      ],
      "metadata": {
        "id": "Zb5zw88NFMXI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer(ABC):\n",
        "    \"\"\"The base class for optimizer.\"\"\"\n",
        "    def __init__(self, learning_rate, layers):\n",
        "        super().__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def update(self):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "bhuFwMuuFiuQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD(Optimizer):\n",
        "    \"\"\"SGD (Stochastic gradient descent) optimizer\"\"\"\n",
        "    def __init__(self, learning_rate, layers):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.layers = layers\n",
        "\n",
        "    def update(self):\n",
        "        for i in range(len(self.layers)):\n",
        "          layer = self.layers[i]\n",
        "          if layer.params is not None:\n",
        "            for key in layer.params.keys():\n",
        "              layer.params[key] -= self.learning_rate * layer.grads[key]"
      ],
      "metadata": {
        "id": "AKcD3el7HsB4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CjWBMDkFuFsf"
      },
      "outputs": [],
      "source": [
        "def train_and_valid(model, optimizer, train_data, train_label, valid_data, valid_label, epochs, batch_size):\n",
        "  \"\"\"\n",
        "  Trains and validates a neural network model.\n",
        "\n",
        "  Args:\n",
        "    model: The neural network model.\n",
        "    optimizer: The optimizer used for training.\n",
        "    train_data: The training data.\n",
        "    train_label: The training labels.\n",
        "    valid_data: The validation data.\n",
        "    valid_label: The validation labels.\n",
        "    epochs: The number of training epochs.\n",
        "    batch_size: The batch size for training.\n",
        "  \"\"\"\n",
        "  for epoch in range(epochs):\n",
        "    # Shuffle training data for each epoch\n",
        "    train_data, train_label = shuffle(train_data, train_label)\n",
        "\n",
        "    # Training loop\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    for data_batch, label_batch in data_generator(train_data, train_label, batch_size):\n",
        "      # print(f\"data_batch shape: {data_batch.shape}\")\n",
        "      # print(f\"label_batch shape: {label_batch.shape}\")\n",
        "      loss, output = model.forward(data_batch, label_batch)\n",
        "      train_loss += loss\n",
        "      train_correct += np.sum(np.argmax(output, axis=1) == label_batch)\n",
        "      model.backward()\n",
        "      optimizer.update()\n",
        "\n",
        "    # Validation loop\n",
        "    valid_loss = 0\n",
        "    valid_correct = 0\n",
        "    for data_batch, label_batch in data_generator(valid_data, valid_label, batch_size):\n",
        "      loss, output = model.forward(data_batch, label_batch)\n",
        "      valid_loss += loss\n",
        "      valid_correct += np.sum(np.argmax(output, axis=1) == label_batch)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_data):.4f}, Train Acc: {train_correct / len(train_data):.4f}, Valid Loss: {valid_loss / len(valid_data):.4f}, Valid Acc: {valid_correct / len(valid_data):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_data, test_label, batch_size=1):\n",
        "  \"\"\"\n",
        "  Tests the neural network model on the test dataset.\n",
        "\n",
        "  Args:\n",
        "    model: The neural network model.\n",
        "    test_data: The test data.\n",
        "    test_label: The test labels.\n",
        "    batch_size: The batch size for testing.\n",
        "  \"\"\"\n",
        "  test_correct = 0\n",
        "  for data_batch, label_batch in data_generator(test_data, test_label, batch_size):\n",
        "    _, output = model.forward(data_batch, label_batch)\n",
        "    test_correct += np.sum(np.argmax(output, axis=1) == label_batch)\n",
        "\n",
        "  print(f'Test Acc: {test_correct / len(test_data):.4f}')"
      ],
      "metadata": {
        "id": "OqTkdNLxFynY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
        "    return int((input_size + 2*pad - filter_size) / stride) + 1"
      ],
      "metadata": {
        "id": "AOrGqLN_ibK4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : The input data, usually images or tensors with 4 dimensions.\n",
        "    filter_h : The height of the filter\n",
        "    filter_w : The width of the filter\n",
        "    stride : The stride of sliding\n",
        "    pad : The number of padding\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    col : 2 dimensional array of shape (N*out_h*out_w, filter_h*filter_w)\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    # out_h = conv_output_size(H, filter_h, stride, pad)\n",
        "    # out_w = conv_output_size(W, filter_w, stride, pad)\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    # print(out_h, out_w)\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "\n",
        "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    col :\n",
        "    input_shape : tuple to show the shape of input data.（example：(10, 1, 28, 28)）\n",
        "    filter_h :\n",
        "    filter_w\n",
        "    stride\n",
        "    pad\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_shape\n",
        "    # out_h = conv_output_size(H, filter_h, stride, pad)\n",
        "    # out_w = conv_output_size(W, filter_w, stride, pad)\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]"
      ],
      "metadata": {
        "id": "6VVB2c79S21t"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2d(Layer):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, pad=0):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size  # Should be a tuple (height, width)\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.params = {}\n",
        "        self.params['W'] = np.random.randn(\n",
        "            out_channels, in_channels, kernel_size[0], kernel_size[1]\n",
        "        )\n",
        "        self.params['b'] = np.random.randn(out_channels)\n",
        "\n",
        "        # Initialize gradients\n",
        "        self.grads = {}\n",
        "        self.grads['W'] = np.zeros_like(self.params['W'])\n",
        "        self.grads['b'] = np.zeros_like(self.params['b'])\n",
        "\n",
        "        # Variables for forward and backward pass\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, b = self.params['W'], self.params['b']\n",
        "        FN, C, FH, FW = W.shape\n",
        "        N, _, H, W_ = x.shape\n",
        "        out_h = (H + 2 * self.pad - FH) // self.stride + 1\n",
        "        out_w = (W_ + 2 * self.pad - FW) // self.stride + 1\n",
        "\n",
        "        # Convert input data to column shape\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = W.reshape(FN, -1).T  # Shape: (C*FH*FW, FN)\n",
        "\n",
        "        # Compute output\n",
        "        out = np.dot(col, col_W) + b  # Shape: (N*out_h*out_w, FN)\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)  # Shape: (N, FN, out_h, out_w)\n",
        "\n",
        "        # Save variables for backward pass\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        # print(f\"dout shape: {dout.shape}\")\n",
        "        W = self.params['W']\n",
        "        FN, C, FH, FW = W.shape\n",
        "\n",
        "        # print(f\"dout shape: {dout.shape}\")\n",
        "        # print(f\"FN: {FN}, C: {C}, FH: {FH}, FW: {FW}\")\n",
        "\n",
        "        # Reshape dout appropriately\n",
        "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
        "\n",
        "        # Compute gradients with respect to bias and weights\n",
        "        self.grads['b'] = np.sum(dout, axis=0)\n",
        "        dW = np.dot(self.col.T, dout)\n",
        "        self.grads['W'] = dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        # Compute gradient with respect to input data\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "        # print(f\"dx shape: {dx.shape}\")\n",
        "\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "6SR85Vj_2FDk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2d(Layer):\n",
        "    \"\"\"\n",
        "    Implements the max pooling operation for convolutional layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size=(2, 2), stride=2, pad=0):\n",
        "        # Pooling parameters\n",
        "        self.pool_h = kernel_size[0]\n",
        "        self.pool_w = kernel_size[1]\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # No learnable parameters in pooling layer\n",
        "        self.params = {}\n",
        "        self.grads = {}\n",
        "\n",
        "        # Variables for forward and backward pass\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "        self.out_h = None\n",
        "        self.out_w = None\n",
        "        self.pool_size = self.pool_h * self.pool_w\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the max pooling layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : Input data of shape (N, C, H, W)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out : Output data after max pooling\n",
        "        \"\"\"\n",
        "        N, C, H, W = x.shape\n",
        "\n",
        "        # Calculate output dimensions\n",
        "        self.out_h = (H + 2 * self.pad - self.pool_h) // self.stride + 1\n",
        "        self.out_w = (W + 2 * self.pad - self.pool_w) // self.stride + 1\n",
        "\n",
        "        # Convert input data to column shape\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        # Reshape to (N, C, out_h, out_w, pool_h * pool_w)\n",
        "        col = col.reshape(N, C, self.out_h, self.out_w, self.pool_size)\n",
        "        # Move channels to the second dimension and flatten\n",
        "        col = col.transpose(0, 2, 3, 1, 4).reshape(-1, self.pool_size)\n",
        "\n",
        "        # Perform max pooling\n",
        "        self.arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "\n",
        "        # Reshape output to (N, C, out_h, out_w)\n",
        "        out = out.reshape(N, self.out_h, self.out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        # Save input and indices for backward pass\n",
        "        self.x = x\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        \"\"\"\n",
        "        Backward pass for the max pooling layer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dout : Upstream derivatives of shape (N, C, out_h, out_w)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dx : Gradient with respect to input data x\n",
        "        \"\"\"\n",
        "        # print(f\"dout shape: {dout.shape}\")\n",
        "        # print(f\"self.arg_max shape: {self.arg_max.shape}\")\n",
        "        N, C, out_h, out_w = dout.shape\n",
        "\n",
        "        # Flatten dout and initialize gradient array\n",
        "        dout_flat = dout.transpose(0, 2, 3, 1).reshape(-1)\n",
        "        dmax = np.zeros((dout_flat.size, self.pool_size))\n",
        "\n",
        "        # Place dout values into appropriate positions in dmax using arg_max\n",
        "        dmax[np.arange(dout_flat.size), self.arg_max] = dout_flat\n",
        "\n",
        "        # Reshape dmax to match the shape needed for col2im\n",
        "        dmax = dmax.reshape(N, out_h, out_w, C, self.pool_size)\n",
        "        dmax = dmax.transpose(0, 3, 1, 2, 4).reshape(N * C * out_h * out_w, self.pool_size)\n",
        "\n",
        "        # Reconstruct the gradient w.r.t input data\n",
        "        dx = col2im(dmax, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        # print(f\"dx shape: {dx.shape}\")\n",
        "\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "TF2yKBg73gsL"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "scrolled": false,
        "id": "ZzHNirM6uFse",
        "outputId": "43fafe4e-5afa-4da6-da48-9e4b35ef92fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[-6.97165763e-01,  2.04338390e+01, -2.22113000e+00, ...,\n",
              "          -6.99341416e+01, -1.94626636e+01, -2.20799297e+01],\n",
              "         [ 2.40498133e+01,  3.07943956e+01,  4.90695542e+01, ...,\n",
              "          -4.93678687e+01, -6.95717891e+00,  3.88145423e-02],\n",
              "         [-2.07383901e+01,  1.20252295e+01,  3.43859480e+01, ...,\n",
              "           7.74732715e+01,  2.13621418e+01,  1.98068746e+01],\n",
              "         ...,\n",
              "         [ 1.79746397e+01, -7.42700354e+00, -4.15900626e+01, ...,\n",
              "           8.51825979e+01, -8.89696416e+01,  8.54719779e+01],\n",
              "         [ 3.55450233e+01, -5.19599667e+01, -1.23699609e+01, ...,\n",
              "           2.53560080e+01, -1.24085472e+01,  1.78211139e+01],\n",
              "         [ 1.29443671e+01,  7.38454946e-01, -1.17029897e+01, ...,\n",
              "           2.60766657e+01,  3.24359677e+01, -1.27285299e+01]],\n",
              "\n",
              "        [[ 4.51360327e+01,  8.37326935e+00,  3.55129490e+01, ...,\n",
              "           2.12434395e+01, -5.95649352e+00, -1.77347578e+01],\n",
              "         [-3.49702851e+01,  1.57964858e-01, -6.35077589e+00, ...,\n",
              "          -3.57931177e+01, -5.43774176e+01, -1.22302033e+01],\n",
              "         [-7.49382321e+00, -1.04393801e+01,  8.13942686e+01, ...,\n",
              "           9.99197424e+01,  1.19746889e+01,  1.93127377e+01],\n",
              "         ...,\n",
              "         [ 4.83730363e+01,  1.27076709e+01, -1.40555825e+02, ...,\n",
              "           5.43307807e+01,  3.02911950e+01,  4.96057807e+00],\n",
              "         [-1.13600925e+01, -5.89934840e+01, -2.64044603e+01, ...,\n",
              "           1.63423381e+01, -3.58632330e+01,  1.95852407e+01],\n",
              "         [ 2.49953327e+01, -5.76748805e+00, -1.90456481e+01, ...,\n",
              "           2.30413386e+01, -3.98907244e+01,  1.05512683e+01]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "c = Conv2d(2, 6, (3, 3), 1, 1)\n",
        "\n",
        "x = np.random.randn(1, 2, 56, 56)\n",
        "\n",
        "out = c.forward(x)\n",
        "\n",
        "c.backward(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KjTp7L2_uFse",
        "outputId": "364c738b-0034-492b-9abc-a8f3106892c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[ 0.        ,  1.43642738,  0.        , ..., -0.0718622 ,\n",
              "           0.5444059 ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 1.46843173,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.47298917],\n",
              "         ...,\n",
              "         [ 0.        ,  0.2413208 ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  1.21410305],\n",
              "         [ 0.96402032,  0.        ,  0.23170115, ...,  0.        ,\n",
              "           0.        ,  0.        ]],\n",
              "\n",
              "        [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  1.24924944,  0.        , ...,  0.        ,\n",
              "           0.98165191,  0.        ],\n",
              "         [ 0.        ,  1.31616059,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         ...,\n",
              "         [ 0.40274051,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.52223542,  0.        , ...,  0.        ,\n",
              "           0.        ,  1.01073974],\n",
              "         [ 0.        ,  0.        ,  0.18997093, ...,  0.        ,\n",
              "           0.        ,  0.        ]]],\n",
              "\n",
              "\n",
              "       [[[ 0.        ,  0.        ,  0.        , ...,  1.17186857,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  1.41180165,  0.84149709, ...,  0.        ,\n",
              "           1.57430869,  0.        ],\n",
              "         [ 1.42342308,  0.        ,  0.        , ...,  0.        ,\n",
              "           1.98045618,  0.        ],\n",
              "         ...,\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  1.23303507],\n",
              "         [-0.29065367,  0.        ,  0.        , ...,  2.59868481,\n",
              "           0.42110894,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ]],\n",
              "\n",
              "        [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.63440448,  0.        ],\n",
              "         [ 0.        ,  1.62312831,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.84904339,  0.        ,  1.14655183, ...,  0.        ,\n",
              "           2.00602444,  0.        ],\n",
              "         ...,\n",
              "         [ 0.29777477,  0.        ,  0.25408753, ...,  0.9815619 ,\n",
              "           0.        ,  1.42270968],\n",
              "         [ 0.        ,  0.        ,  1.90871664, ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.29203843,  0.        , ...,  1.29164596,\n",
              "           0.        ,  0.51351465]]],\n",
              "\n",
              "\n",
              "       [[[ 0.86536514,  0.        ,  0.        , ...,  0.7013651 ,\n",
              "           0.        ,  0.73823611],\n",
              "         [ 0.        ,  0.        ,  1.73676993, ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.6639011 , ...,  0.76056275,\n",
              "           1.18878327,  0.        ],\n",
              "         ...,\n",
              "         [ 1.36034592,  0.        ,  0.        , ...,  0.88895819,\n",
              "           0.8301393 ,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.81786992, ...,  0.        ,\n",
              "           0.        ,  0.99689785],\n",
              "         [-0.4997529 ,  0.        ,  0.        , ..., -0.97780646,\n",
              "           0.        ,  0.        ]],\n",
              "\n",
              "        [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         [ 1.40025229,  0.        ,  1.75751456, ...,  0.        ,\n",
              "           0.        ,  0.48222365],\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         ...,\n",
              "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "           0.63302784,  0.        ],\n",
              "         [ 0.        ,  0.0532008 ,  0.        , ...,  1.32867901,\n",
              "           0.20050355,  0.        ],\n",
              "         [ 0.        ,  0.        ,  0.92003819, ...,  0.        ,\n",
              "           0.        ,  0.        ]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "m = MaxPool2d((2, 2), 2)\n",
        "\n",
        "x = np.random.randn(3, 2, 56, 56)\n",
        "\n",
        "out = m.forward(x)\n",
        "\n",
        "m.backward(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining droput layer"
      ],
      "metadata": {
        "id": "boyPc1dS4swe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout(Layer):\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.mask = None\n",
        "        self.params = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.dropout_rate == 0:\n",
        "            return x\n",
        "\n",
        "        # Creating dropout mask (1 = keep, 0 = drop)\n",
        "        self.mask = np.random.binomial(1, 1-self.dropout_rate, size=x.shape) / (1-self.dropout_rate)\n",
        "        return x * self.mask\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n"
      ],
      "metadata": {
        "id": "gDkKP8Eu4sJi"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randn(3, 2, 56, 56)\n",
        "drop_out = Dropout(0.5)\n",
        "out = drop_out.forward(x)\n",
        "drop_out.backward(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll39JwQS7tz6",
        "outputId": "73f34045-4e57-467a-d13a-9c280227fdc1"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[ 0.        ,  0.        , -2.63244907, ..., -0.        ,\n",
              "           5.0334409 ,  0.        ],\n",
              "         [-0.        ,  5.05632502, -0.        , ..., -3.73094382,\n",
              "           7.57012586, -0.        ],\n",
              "         [ 5.58253637, -0.        ,  0.07964916, ..., -3.66704961,\n",
              "          -5.59414865,  1.95333982],\n",
              "         ...,\n",
              "         [-0.        , -0.        , -0.        , ...,  3.6451841 ,\n",
              "           0.4541713 ,  0.        ],\n",
              "         [ 0.        , -0.23655534,  0.55255161, ...,  0.96871255,\n",
              "           1.65212866,  6.75158352],\n",
              "         [ 0.        ,  0.        ,  2.60283542, ...,  0.        ,\n",
              "          -1.56689275, -0.        ]],\n",
              "\n",
              "        [[ 0.27597265,  6.13164271, -4.33439464, ..., -0.20557832,\n",
              "          -0.        ,  1.00815956],\n",
              "         [ 1.35981663, -0.        , -0.        , ...,  0.        ,\n",
              "          -0.        , -4.99812806],\n",
              "         [ 4.11029313,  0.        , -0.        , ..., -0.        ,\n",
              "          -2.05533135, -0.        ],\n",
              "         ...,\n",
              "         [-0.14821361,  2.05143705, -0.7449079 , ..., -0.        ,\n",
              "           4.29448987,  3.86006806],\n",
              "         [-5.22233572, -0.34932805,  0.        , ..., -0.        ,\n",
              "           0.        ,  2.02581201],\n",
              "         [-1.03539948,  0.13202388, -0.58805263, ..., -0.        ,\n",
              "          -0.        , -0.        ]]],\n",
              "\n",
              "\n",
              "       [[[-4.60580047,  0.        , -0.        , ..., -2.85224965,\n",
              "           0.        , -3.54224834],\n",
              "         [-3.33354862, -3.15003619,  0.        , ...,  7.71169864,\n",
              "           0.72192264,  0.        ],\n",
              "         [ 8.50221949,  3.30856258, -0.        , ..., -0.        ,\n",
              "          -0.        , -0.        ],\n",
              "         ...,\n",
              "         [ 0.        ,  2.79727199,  0.        , ..., -2.77799049,\n",
              "           0.        ,  5.60367088],\n",
              "         [-0.        ,  2.1238057 ,  0.        , ...,  0.        ,\n",
              "           0.        , -3.17092505],\n",
              "         [ 0.        , -3.26812984,  4.27102525, ..., -1.97955227,\n",
              "          -2.7885997 ,  0.        ]],\n",
              "\n",
              "        [[-5.48464078, -0.        ,  1.4960685 , ...,  4.99211057,\n",
              "          -7.0731691 ,  0.        ],\n",
              "         [-0.        , -0.        , -0.70033546, ..., -1.5095167 ,\n",
              "           0.        ,  1.79931418],\n",
              "         [-0.45626151,  0.        , -0.        , ..., -0.        ,\n",
              "          -0.        ,  0.81694273],\n",
              "         ...,\n",
              "         [ 0.        ,  5.82869051,  3.46684417, ..., -0.        ,\n",
              "          -0.77762047,  0.        ],\n",
              "         [ 2.60798072,  3.52886089, -0.        , ...,  0.        ,\n",
              "           1.28296388, -0.        ],\n",
              "         [ 1.01652362, -0.        ,  3.82889618, ..., -0.        ,\n",
              "          -0.        ,  3.67080192]]],\n",
              "\n",
              "\n",
              "       [[[-0.        , -1.80414466, -4.79042812, ...,  2.09387627,\n",
              "           0.        , -1.77267886],\n",
              "         [ 5.29357707,  0.        , -0.        , ..., -7.89019483,\n",
              "          -0.        , -6.137574  ],\n",
              "         [-1.3582559 ,  8.33448232, -0.        , ..., -0.        ,\n",
              "          -2.65516245, -0.        ],\n",
              "         ...,\n",
              "         [ 5.88734351,  0.4825113 ,  1.50487301, ...,  0.        ,\n",
              "          -1.2199749 ,  0.        ],\n",
              "         [-3.95678684,  0.        ,  0.        , ..., -0.        ,\n",
              "           0.        ,  3.15052754],\n",
              "         [-0.        , -7.9784632 ,  4.4578143 , ...,  0.36998353,\n",
              "           0.        , -3.82382578]],\n",
              "\n",
              "        [[ 0.        ,  1.39517324,  5.5889662 , ..., -0.        ,\n",
              "           1.7190699 ,  0.        ],\n",
              "         [-0.44069388,  0.        , -2.08424404, ...,  9.20011336,\n",
              "           0.73391602,  4.73153008],\n",
              "         [-0.        ,  3.42864942, -0.        , ...,  0.        ,\n",
              "           0.        ,  0.        ],\n",
              "         ...,\n",
              "         [ 0.        , -0.        ,  0.        , ...,  2.14671637,\n",
              "          -0.        , -0.        ],\n",
              "         [ 0.        ,  1.10042614, -0.        , ...,  0.        ,\n",
              "           0.        ,  3.28122695],\n",
              "         [ 1.89380963, -2.19889833, -0.        , ..., -1.88530137,\n",
              "          -0.        , -0.        ]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalAvgPool2d(Layer):\n",
        "    def __init__(self, keepdims=False):\n",
        "        self.params = None\n",
        "        self.x_shape = None\n",
        "        self.keepdims = keepdims\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x_shape = x.shape\n",
        "        return np.mean(x, axis=(2, 3), keepdims=self.keepdims)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        batch_size, channels, height, width = self.x_shape\n",
        "        if not self.keepdims:\n",
        "            dout = dout.reshape(batch_size, channels, 1, 1)\n",
        "        dx = np.ones(self.x_shape) * dout / (height * width)\n",
        "        return dx"
      ],
      "metadata": {
        "id": "xf53ptHk7eMG"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randn(3, 2, 56, 56)\n",
        "global_avg_pool = GlobalAvgPool2d()\n",
        "out = global_avg_pool.forward(x)\n",
        "print(out.shape)\n",
        "global_avg_pool.backward(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xisIBRb750P",
        "outputId": "435a878c-61a2-4a0b-cba6-2526a99c19b4"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[-6.10764264e-07, -6.10764264e-07, -6.10764264e-07, ...,\n",
              "          -6.10764264e-07, -6.10764264e-07, -6.10764264e-07],\n",
              "         [-6.10764264e-07, -6.10764264e-07, -6.10764264e-07, ...,\n",
              "          -6.10764264e-07, -6.10764264e-07, -6.10764264e-07],\n",
              "         [-6.10764264e-07, -6.10764264e-07, -6.10764264e-07, ...,\n",
              "          -6.10764264e-07, -6.10764264e-07, -6.10764264e-07],\n",
              "         ...,\n",
              "         [-6.10764264e-07, -6.10764264e-07, -6.10764264e-07, ...,\n",
              "          -6.10764264e-07, -6.10764264e-07, -6.10764264e-07],\n",
              "         [-6.10764264e-07, -6.10764264e-07, -6.10764264e-07, ...,\n",
              "          -6.10764264e-07, -6.10764264e-07, -6.10764264e-07],\n",
              "         [-6.10764264e-07, -6.10764264e-07, -6.10764264e-07, ...,\n",
              "          -6.10764264e-07, -6.10764264e-07, -6.10764264e-07]],\n",
              "\n",
              "        [[ 7.38793172e-07,  7.38793172e-07,  7.38793172e-07, ...,\n",
              "           7.38793172e-07,  7.38793172e-07,  7.38793172e-07],\n",
              "         [ 7.38793172e-07,  7.38793172e-07,  7.38793172e-07, ...,\n",
              "           7.38793172e-07,  7.38793172e-07,  7.38793172e-07],\n",
              "         [ 7.38793172e-07,  7.38793172e-07,  7.38793172e-07, ...,\n",
              "           7.38793172e-07,  7.38793172e-07,  7.38793172e-07],\n",
              "         ...,\n",
              "         [ 7.38793172e-07,  7.38793172e-07,  7.38793172e-07, ...,\n",
              "           7.38793172e-07,  7.38793172e-07,  7.38793172e-07],\n",
              "         [ 7.38793172e-07,  7.38793172e-07,  7.38793172e-07, ...,\n",
              "           7.38793172e-07,  7.38793172e-07,  7.38793172e-07],\n",
              "         [ 7.38793172e-07,  7.38793172e-07,  7.38793172e-07, ...,\n",
              "           7.38793172e-07,  7.38793172e-07,  7.38793172e-07]]],\n",
              "\n",
              "\n",
              "       [[[-1.23501132e-06, -1.23501132e-06, -1.23501132e-06, ...,\n",
              "          -1.23501132e-06, -1.23501132e-06, -1.23501132e-06],\n",
              "         [-1.23501132e-06, -1.23501132e-06, -1.23501132e-06, ...,\n",
              "          -1.23501132e-06, -1.23501132e-06, -1.23501132e-06],\n",
              "         [-1.23501132e-06, -1.23501132e-06, -1.23501132e-06, ...,\n",
              "          -1.23501132e-06, -1.23501132e-06, -1.23501132e-06],\n",
              "         ...,\n",
              "         [-1.23501132e-06, -1.23501132e-06, -1.23501132e-06, ...,\n",
              "          -1.23501132e-06, -1.23501132e-06, -1.23501132e-06],\n",
              "         [-1.23501132e-06, -1.23501132e-06, -1.23501132e-06, ...,\n",
              "          -1.23501132e-06, -1.23501132e-06, -1.23501132e-06],\n",
              "         [-1.23501132e-06, -1.23501132e-06, -1.23501132e-06, ...,\n",
              "          -1.23501132e-06, -1.23501132e-06, -1.23501132e-06]],\n",
              "\n",
              "        [[ 1.91636276e-06,  1.91636276e-06,  1.91636276e-06, ...,\n",
              "           1.91636276e-06,  1.91636276e-06,  1.91636276e-06],\n",
              "         [ 1.91636276e-06,  1.91636276e-06,  1.91636276e-06, ...,\n",
              "           1.91636276e-06,  1.91636276e-06,  1.91636276e-06],\n",
              "         [ 1.91636276e-06,  1.91636276e-06,  1.91636276e-06, ...,\n",
              "           1.91636276e-06,  1.91636276e-06,  1.91636276e-06],\n",
              "         ...,\n",
              "         [ 1.91636276e-06,  1.91636276e-06,  1.91636276e-06, ...,\n",
              "           1.91636276e-06,  1.91636276e-06,  1.91636276e-06],\n",
              "         [ 1.91636276e-06,  1.91636276e-06,  1.91636276e-06, ...,\n",
              "           1.91636276e-06,  1.91636276e-06,  1.91636276e-06],\n",
              "         [ 1.91636276e-06,  1.91636276e-06,  1.91636276e-06, ...,\n",
              "           1.91636276e-06,  1.91636276e-06,  1.91636276e-06]]],\n",
              "\n",
              "\n",
              "       [[[-4.35820318e-06, -4.35820318e-06, -4.35820318e-06, ...,\n",
              "          -4.35820318e-06, -4.35820318e-06, -4.35820318e-06],\n",
              "         [-4.35820318e-06, -4.35820318e-06, -4.35820318e-06, ...,\n",
              "          -4.35820318e-06, -4.35820318e-06, -4.35820318e-06],\n",
              "         [-4.35820318e-06, -4.35820318e-06, -4.35820318e-06, ...,\n",
              "          -4.35820318e-06, -4.35820318e-06, -4.35820318e-06],\n",
              "         ...,\n",
              "         [-4.35820318e-06, -4.35820318e-06, -4.35820318e-06, ...,\n",
              "          -4.35820318e-06, -4.35820318e-06, -4.35820318e-06],\n",
              "         [-4.35820318e-06, -4.35820318e-06, -4.35820318e-06, ...,\n",
              "          -4.35820318e-06, -4.35820318e-06, -4.35820318e-06],\n",
              "         [-4.35820318e-06, -4.35820318e-06, -4.35820318e-06, ...,\n",
              "          -4.35820318e-06, -4.35820318e-06, -4.35820318e-06]],\n",
              "\n",
              "        [[-7.39824624e-06, -7.39824624e-06, -7.39824624e-06, ...,\n",
              "          -7.39824624e-06, -7.39824624e-06, -7.39824624e-06],\n",
              "         [-7.39824624e-06, -7.39824624e-06, -7.39824624e-06, ...,\n",
              "          -7.39824624e-06, -7.39824624e-06, -7.39824624e-06],\n",
              "         [-7.39824624e-06, -7.39824624e-06, -7.39824624e-06, ...,\n",
              "          -7.39824624e-06, -7.39824624e-06, -7.39824624e-06],\n",
              "         ...,\n",
              "         [-7.39824624e-06, -7.39824624e-06, -7.39824624e-06, ...,\n",
              "          -7.39824624e-06, -7.39824624e-06, -7.39824624e-06],\n",
              "         [-7.39824624e-06, -7.39824624e-06, -7.39824624e-06, ...,\n",
              "          -7.39824624e-06, -7.39824624e-06, -7.39824624e-06],\n",
              "         [-7.39824624e-06, -7.39824624e-06, -7.39824624e-06, ...,\n",
              "          -7.39824624e-06, -7.39824624e-06, -7.39824624e-06]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding batch normalization layer"
      ],
      "metadata": {
        "id": "_RjWs7F596Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm2d(Layer):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.9):\n",
        "        self.params = {}\n",
        "        self.params['gamma'] = np.ones(num_features)\n",
        "        self.params['beta'] = np.zeros(num_features)\n",
        "\n",
        "        self.grads = {}\n",
        "        self.grads['gamma'] = np.zeros_like(self.params['gamma'])\n",
        "        self.grads['beta'] = np.zeros_like(self.params['beta'])\n",
        "\n",
        "        self.eps = eps\n",
        "        self.m = momentum\n",
        "\n",
        "\n",
        "        self.rmean = np.zeros(num_features)\n",
        "        self.rvar = np.ones(num_features)\n",
        "\n",
        "\n",
        "        self.x_norm = None\n",
        "        self.x = None\n",
        "        self.batch_mean = None\n",
        "        self.batch_var = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        self.x = x\n",
        "\n",
        "        x_reshaped = x.transpose(0, 2, 3, 1).reshape(-1, C)\n",
        "\n",
        "        self.batch_mean = np.mean(x_reshaped, axis=0)\n",
        "        self.batch_var = np.var(x_reshaped, axis=0)\n",
        "\n",
        "\n",
        "        self.rmean = self.m * self.rmean + (1 - self.m) * self.batch_mean\n",
        "        self.rvar = self.m * self.rvar + (1 - self.m) * self.batch_var\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x_reshaped - self.batch_mean) / np.sqrt(self.batch_var + self.eps)\n",
        "        self.x_norm = x_norm\n",
        "\n",
        "        # Scale and shift\n",
        "        out = self.params['gamma'] * x_norm + self.params['beta']\n",
        "\n",
        "        # Reshape back\n",
        "        out = out.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, C, H, W = dout.shape\n",
        "\n",
        "        dout_reshaped = dout.transpose(0, 2, 3, 1).reshape(-1, C)\n",
        "\n",
        "\n",
        "        self.grads['gamma'] = np.sum(dout_reshaped * self.x_norm, axis=0)\n",
        "        self.grads['beta'] = np.sum(dout_reshaped, axis=0)\n",
        "\n",
        "        dx_norm = dout_reshaped * self.params['gamma']\n",
        "\n",
        "\n",
        "        dvar = np.sum(dx_norm * (self.x.transpose(0, 2, 3, 1).reshape(-1, C) - self.batch_mean) *\n",
        "                      -0.5 * (self.batch_var + self.eps)**(-1.5), axis=0)\n",
        "\n",
        "        N_reshaped = N * H * W\n",
        "        dmean = np.sum(dx_norm * -1.0 / np.sqrt(self.batch_var + self.eps), axis=0) + \\\n",
        "                dvar * np.mean(-2.0 * (self.x.transpose(0, 2, 3, 1).reshape(-1, C) - self.batch_mean), axis=0)\n",
        "\n",
        "        dx_reshaped = dx_norm / np.sqrt(self.batch_var + self.eps) + \\\n",
        "                     dvar * 2.0 * (self.x.transpose(0, 2, 3, 1).reshape(-1, C) - self.batch_mean) / N_reshaped + \\\n",
        "                     dmean / N_reshaped\n",
        "\n",
        "        dx = dx_reshaped.reshape(N, H, W, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "INB9HKHT98HL"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding ADAM optimizer\n",
        "\n",
        "referred from: https://www.datacamp.com/tutorial/adam-optimizer-tutorial"
      ],
      "metadata": {
        "id": "udjVi8kb-wp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam(Optimizer):\n",
        "    \"\"\"Adam optimizer\"\"\"\n",
        "    def __init__(self, learning_rate, layers, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.layers = layers\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        self.t = 0\n",
        "\n",
        "        # Initialize moment vectors\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if layer.params is not None:\n",
        "                for key in layer.params.keys():\n",
        "                    self.m[(i, key)] = np.zeros_like(layer.params[key])\n",
        "                    self.v[(i, key)] = np.zeros_like(layer.params[key])\n",
        "\n",
        "    def update(self):\n",
        "        self.t += 1\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if layer.params is not None:\n",
        "                for key in layer.params.keys():\n",
        "                    grad = layer.grads[key]\n",
        "\n",
        "                    self.m[(i, key)] = self.beta1 * self.m[(i, key)] + (1 - self.beta1) * grad\n",
        "\n",
        "                    self.v[(i, key)] = self.beta2 * self.v[(i, key)] + (1 - self.beta2) * (grad ** 2)\n",
        "\n",
        "                    m_corrected = self.m[(i, key)] / (1 - self.beta1 ** self.t)\n",
        "\n",
        "                    v_corrected = self.v[(i, key)] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "                    layer.params[key] -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.eps)"
      ],
      "metadata": {
        "id": "IakIOlGW-yUU"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "v4Dr_C7EuFse"
      },
      "outputs": [],
      "source": [
        "# ========== OLD NETWORK ==========\n",
        "\n",
        "# class CNN:\n",
        "#     def __init__(self):\n",
        "#         self.layers = []\n",
        "#         self.layers.append(Conv2d(in_channels=1, out_channels=6, kernel_size=(3, 3), stride=1, pad=0))\n",
        "#         self.layers.append(ReLU())\n",
        "#         self.layers.append(MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "#         self.layers.append(Conv2d(in_channels=6, out_channels=16, kernel_size=(3, 3), stride=1, pad=0))\n",
        "#         self.layers.append(ReLU())\n",
        "#         self.layers.append(MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "#         self.layers.append(Linear(input_dim=400, output_dim=120))\n",
        "#         self.layers.append(Linear(input_dim=120, output_dim=84))\n",
        "#         self.layers.append(Linear(input_dim=84, output_dim=10))\n",
        "#         self.layers.append(SoftmaxWithCrossEntropyLoss())\n",
        "\n",
        "#     def forward(self, x, y):\n",
        "\n",
        "#         batch_size = x.shape[0]\n",
        "\n",
        "#         y = label_to_one_hot(y, 10)\n",
        "\n",
        "#         split_index = 6\n",
        "#         for layer in self.layers[:split_index]:\n",
        "#             x = layer.forward(x)\n",
        "#         self.shape_before_flatten = x.shape\n",
        "#         x = x.reshape(batch_size, -1)\n",
        "#         print(self.shape_before_flatten, x.shape)\n",
        "#         x = self.layers[-4].forward(x)\n",
        "#         x = self.layers[-3].forward(x)\n",
        "#         x = self.layers[-2].forward(x)\n",
        "\n",
        "#         return self.layers[-1].forward(x, y)\n",
        "\n",
        "\n",
        "#     def backward(self):\n",
        "#         dout = 1\n",
        "#         for idx, layer in enumerate(self.layers[::-1]):\n",
        "#             if idx != 4:\n",
        "#                 dout = layer.backward(dout)\n",
        "#             else:\n",
        "#                 dout = dout.reshape(self.shape_before_flatten)\n",
        "#                 dout = layer.backward(dout)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "        self.layers.append(Conv2d(in_channels=1, out_channels=6, kernel_size=(3, 3), stride=1, pad=0))\n",
        "        self.layers.append(BatchNorm2d(num_features=6))\n",
        "        self.layers.append(ReLU())\n",
        "        self.layers.append(Dropout(dropout_rate=0.30))\n",
        "        self.layers.append(MaxPool2d(kernel_size=(2, 2), stride=2))\n",
        "\n",
        "        self.layers.append(Conv2d(in_channels=6, out_channels=16, kernel_size=(3, 3), stride=1, pad=0))\n",
        "        self.layers.append(ReLU())\n",
        "        self.layers.append(Dropout(dropout_rate=0.30))\n",
        "        self.layers.append(GlobalAvgPool2d())\n",
        "\n",
        "        self.layers.append(Linear(input_dim=16, output_dim=32))\n",
        "        self.layers.append(ReLU())\n",
        "        self.layers.append(Dropout(dropout_rate=0.5))\n",
        "        self.layers.append(Linear(input_dim=32, output_dim=10))\n",
        "        self.layers.append(SoftmaxWithCrossEntropyLoss())\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        y = label_to_one_hot(y, 10)\n",
        "\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return self.layers[-1].forward(x, y)\n",
        "\n",
        "\n",
        "    def backward(self):\n",
        "        dout = 1\n",
        "        for idx, layer in enumerate(self.layers[::-1]):\n",
        "            dout = layer.backward(dout)"
      ],
      "metadata": {
        "id": "nCYTl-HtBMCA"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "4CjBM0EOuFsf"
      },
      "outputs": [],
      "source": [
        "learning_rate = 5e-3\n",
        "epochs = 10\n",
        "batch_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "DQa03sv2uFsf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "739ba504-e906-4c57-d67c-188970da10f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10, Train Loss: 0.0087, Train Acc: 0.2056, Valid Loss: 0.0067, Valid Acc: 0.3541\n",
            "Epoch: 2/10, Train Loss: 0.0057, Train Acc: 0.4763, Valid Loss: 0.0050, Valid Acc: 0.5415\n",
            "Epoch: 3/10, Train Loss: 0.0045, Train Acc: 0.5979, Valid Loss: 0.0041, Valid Acc: 0.6349\n",
            "Epoch: 4/10, Train Loss: 0.0040, Train Acc: 0.6511, Valid Loss: 0.0038, Valid Acc: 0.6705\n",
            "Epoch: 5/10, Train Loss: 0.0036, Train Acc: 0.6853, Valid Loss: 0.0035, Valid Acc: 0.7026\n",
            "Epoch: 6/10, Train Loss: 0.0033, Train Acc: 0.7247, Valid Loss: 0.0031, Valid Acc: 0.7432\n",
            "Epoch: 7/10, Train Loss: 0.0030, Train Acc: 0.7476, Valid Loss: 0.0029, Valid Acc: 0.7568\n",
            "Epoch: 8/10, Train Loss: 0.0028, Train Acc: 0.7678, Valid Loss: 0.0028, Valid Acc: 0.7719\n",
            "Epoch: 9/10, Train Loss: 0.0026, Train Acc: 0.7827, Valid Loss: 0.0026, Valid Acc: 0.7883\n",
            "Epoch: 10/10, Train Loss: 0.0026, Train Acc: 0.7920, Valid Loss: 0.0025, Valid Acc: 0.7957\n"
          ]
        }
      ],
      "source": [
        "cnn_model = CNN()\n",
        "adam_optimizer = Adam(learning_rate, cnn_model.layers)\n",
        "\n",
        "train_and_valid(cnn_model, adam_optimizer, train_data, train_label, valid_data, valid_label, epochs, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "P4cimcZ0uFsf",
        "outputId": "a077a03c-b1ef-450f-e476-6e67f87089a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Acc: 0.8015\n"
          ]
        }
      ],
      "source": [
        "test(cnn_model, test_data, test_label)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}